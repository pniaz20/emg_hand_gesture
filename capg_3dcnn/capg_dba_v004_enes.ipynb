{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition with CAPG DB-a Dataset Using 3D CNN with EMGNet Architecture (one subject for testing)\n",
    "\n",
    "In this preliminary effort, we will try to perform hand gesture recognition on CAPG DBA dataset.\n",
    "We will use the EMGNet architecture and training procedure, but instead of CWT, we will use 3D CNN on sequences of 2D images.\n",
    "\n",
    "In this version:\n",
    "\n",
    "- EMG data is normalized with the recorded MVC data\n",
    "- The **EMGNet** architecture will be used, along with the training procedure.\n",
    "- A **3D CNN** architecture will be adopted into the EMGNet architecture.\n",
    "- **Raw EMG data** will be used, there will be no preproccessing or feature engineering.\n",
    "- **Training data:** 17 subjects\n",
    "- **Test data:** 1 subject\n",
    "- K-fold cross-validation will be performed.\n",
    "\n",
    "**NOTE** This code has been tested with:\n",
    "```\n",
    "    numpy version:        1.23.5\n",
    "    scipy version:        1.9.3\n",
    "    sklearn version:      1.2.0\n",
    "    seaborn version:      0.12.1\n",
    "    pandas version:       1.5.2\n",
    "    torch version:        1.12.1+cu113\n",
    "    matplotlib version:   3.6.2\n",
    "    CUDA version:         11.2\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Preliminaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "direc = os.getcwd()\n",
    "print(\"Current Working Directory is: \", direc)\n",
    "KUACC = False\n",
    "if \"scratch\" in direc: # We are using the cluster\n",
    "    KUACC = True\n",
    "    homedir = os.path.expanduser(\"~\")\n",
    "    os.chdir(os.path.join(homedir,\"comp541-project/capg_3dcnn/\"))\n",
    "    direc = os.getcwd()\n",
    "    print(\"Current Working Directory is now: \", direc)\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../data/\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets_torch import *\n",
    "from models_torch import *\n",
    "from utils_torch import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import statistics\n",
    "import json\n",
    "from IPython.display import display\n",
    "#from cwt import calculate_wavelet_vector, calculate_wavelet_dataset\n",
    "\n",
    "# Print versions\n",
    "print(\"numpy version:       \", np.__version__)\n",
    "print(\"scipy version:       \", sp.__version__)\n",
    "print(\"sklearn version:     \", sklearn.__version__)\n",
    "print(\"seaborn version:     \", sns.__version__)\n",
    "print(\"pandas version:      \", pd.__version__)\n",
    "print(\"torch version:       \", torch.__version__)\n",
    "print(\"matplotlib version:  \", matplotlib.__version__)\n",
    "\n",
    "\n",
    "# Checking to see if CUDA is available for us\n",
    "print(\"Checking to see if PyTorch recognizes GPU...\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Whether to use latex rendering in plots throughout the notebook\n",
    "USE_TEX = KUACC \n",
    "FONT_SIZE = 12\n",
    "\n",
    "# Setting matplotlib plotting variables\n",
    "if USE_TEX:\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.size\": FONT_SIZE,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Computer Modern\"\n",
    "    })\n",
    "else:\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.size\": FONT_SIZE,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": \"Times New Roman\"\n",
    "    })\n",
    "\n",
    "# Do not plot figures inline (only useful for cluster)\n",
    "# %matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Hyperparameters and Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General settings of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_study = {\n",
    "    'code':'capg_3dcnn/capg_dba_v004',\n",
    "    'package':'torch',\n",
    "    'dataset':'capg',\n",
    "    'subdataset':'dba',\n",
    "    \"training_accuracies\": [],\n",
    "    \"validation_accuracies\": [],\n",
    "    \"testset_accuracies\": [],\n",
    "    \"history_training_loss\": [],\n",
    "    \"history_training_metrics\": [],\n",
    "    \"history_validation_loss\": [],\n",
    "    \"history_validation_metrics\": [],\n",
    "    \"preprocessing\":None,\n",
    "    \"feature_engineering\":None,\n",
    "    \"k_fold_mode\":\"1 subject for testing\",\n",
    "    \"global_downsampling\":10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"model_name\": autoname(\"capg_3dcnn_dba_v004\"),\n",
    "    # General hyperparameters\n",
    "    \"in_features\": 128,\n",
    "    \"out_features\": 1,\n",
    "    \"input_shape\":[1,64,8,16],\n",
    "    \"output_shape\":[8],\n",
    "    # Sequence hyperparameters\n",
    "    \"in_seq_len_sec\": 0.16,\n",
    "    \"out_seq_len_sec\": 0,\n",
    "    \"data_sampling_rate_Hz\": 1000.0,\n",
    "    \"data_downsampling\": 5,\n",
    "    \"sequence_downsampling\": 1,\n",
    "    \"in_seq_len\": 0,\n",
    "    \"out_seq_len\": 0,\n",
    "    # Convolution blocks\n",
    "    \"num_conv_blocks\": 4,\n",
    "    \"conv_dim\": 3,\n",
    "    \"conv_params\": None,\n",
    "    \"conv_channels\": [16, 32, 32, 64],\n",
    "    \"conv_kernel_size\": 3,\n",
    "    \"conv_padding\": \"same\",\n",
    "    \"conv_stride\": 1,\n",
    "    \"conv_dilation\": 1,\n",
    "    \"conv_activation\": \"ReLU\",\n",
    "    \"conv_activation_params\": None,#{\"negative_slope\": 0.1},\n",
    "    \"conv_norm_layer_type\": \"BatchNorm\",\n",
    "    \"conv_norm_layer_position\": \"before\",\n",
    "    \"conv_norm_layer_params\": {'momentum':0.99, 'eps':1.0e-8},\n",
    "    \"conv_dropout\": None,\n",
    "    \"pool_type\": [None, None, None, \"AdaptiveAvg\"],\n",
    "    \"pool_kernel_size\": 2,\n",
    "    \"pool_padding\": 0,\n",
    "    \"pool_stride\": 1,\n",
    "    \"pool_dilation\": 1,\n",
    "    \"pool_params\": None,\n",
    "    \"min_image_size\": 1,\n",
    "    \"adaptive_pool_output_size\": [1,1,1],\n",
    "    # Fully connected blocks\n",
    "    \"dense_width\": \"auto\",\n",
    "    \"dense_depth\": 0,\n",
    "    \"dense_activation\": \"ReLU\",\n",
    "    \"dense_activation_params\": None,\n",
    "    \"output_activation\": None,\n",
    "    \"output_activation_params\": None,\n",
    "    \"dense_norm_layer_type\": None,\n",
    "    \"dense_norm_layer_position\": None,\n",
    "    \"dense_norm_layer_params\": {'momentum':0.99, 'eps':1.0e-8},\n",
    "    \"dense_dropout\": None,\n",
    "    # Training procedure\n",
    "    \"l2_reg\": 0.0001,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 60,\n",
    "    \"validation_data\": [0.05,'trainset'],\n",
    "    \"validation_tolerance_epochs\": 1000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"learning_rate_decay_gamma\": 0.9,\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"optimizer_params\": None\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/CAPG/parquet\"\n",
    "def load_single_capg_dataset(data_dir, db_str:str=\"dba\"):\n",
    "    data_lst = []\n",
    "    for i,file in enumerate(os.listdir(data_dir)):\n",
    "        if file.endswith(\".parquet\") and db_str in file:\n",
    "            print(\"Loading file: \", file)\n",
    "            data_lst.append(pd.read_parquet(os.path.join(data_dir, file)))\n",
    "    data = pd.concat(data_lst, axis=0, ignore_index=True)\n",
    "    return data\n",
    "dba_tot = load_single_capg_dataset(data_dir, db_str=\"dba\")\n",
    "dba_mvc = dba_tot.loc[dba_tot[\"gesture\"].isin([100, 101])]\n",
    "dba = dba_tot.loc[~dba_tot[\"gesture\"].isin([100, 101])]\n",
    "print(\"dba_tot shape: \", dba_tot.shape)\n",
    "print(\"dba_mvc shape: \", dba_mvc.shape)\n",
    "print(\"dba shape: \", dba.shape)\n",
    "print(\"Columns: \")\n",
    "print(dba_tot.columns)\n",
    "print(\"Description: \")\n",
    "print(dba.iloc[:,:3].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize EMG Data\n",
    "\n",
    "Here the recorded MVC values will be used for normalizaing EMG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mvc = dba_mvc.iloc[:,3:].max(axis=0)\n",
    "del dba_mvc\n",
    "# print(\"max_mvc for 5 first channels: \")\n",
    "# print(max_mvc[:5])\n",
    "# print(\"shape of max_mvc: \", max_mvc.shape)\n",
    "# print(\"max of dba before normalization: (first five)\")\n",
    "# print(dba.iloc[:,3:].max(axis=0)[:5])\n",
    "dba.iloc[:,3:] = dba.iloc[:,3:].div(max_mvc, axis=1)\n",
    "# print(\"max of dba_norm after normalization: \")\n",
    "# print(dba_norm.iloc[:,3:].max(axis=0)[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Pre-Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMGNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMGNet(PyTorchSmartModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(EMGNet, self).__init__(hparams)\n",
    "        self.prep_block = nn.Sequential(\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main_block = Conv_Network(hparams)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep_block(x)\n",
    "        x = self.main_block(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Training for 17 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define input columns\n",
    "input_cols = list(dba.iloc[:,3:].columns)\n",
    "\n",
    "# Hard-code total number of subjects\n",
    "\n",
    "k = 18\n",
    "num_subjects = dba['subject'].nunique()\n",
    "\n",
    "ds = k_fold_study['global_downsampling']\n",
    "\n",
    "\n",
    "print(\"\\n#################################################################\")\n",
    "print(\"Using subject %d for testing ...\" % (k))\n",
    "print(\"#################################################################\\n\")\n",
    "subj_for_testing = [k]\n",
    "\n",
    "# Un-Correct the output feature count (this is buggy behavior and should be fixed)\n",
    "hparams['out_features'] = 1\n",
    "\n",
    "# Get processed data cell\n",
    "# CWT: N x C x L --> N x C x H x L\n",
    "print(\"Generating data cell ...\")\n",
    "data_processed = generate_cell_array(\n",
    "    dba, hparams,\n",
    "    subjects_column=\"subject\", conditions_column=\"gesture\", trials_column=\"trial\",\n",
    "    input_cols=input_cols, output_cols=[\"gesture\"], specific_conditions=None,\n",
    "    input_preprocessor=None,\n",
    "    output_preprocessor=None,\n",
    "    # Convert N x L x C data to N x C x L and then to N x C' x D x H x W where C'=1, D=L, H=8, W=16\n",
    "    input_postprocessor=lambda arr: arr.reshape(arr.shape[0], 1, arr.shape[1], 8, 16),\n",
    "    output_postprocessor = lambda arr:(arr-1).squeeze(), # torch CrossEntropyLoss needs (N,) array of 0-indexed class labels\n",
    "    subjects_for_testing=subj_for_testing, \n",
    "    trials_for_testing=None,\n",
    "    input_scaling=False, output_scaling=False, input_forward_facing=True, output_forward_facing=True, \n",
    "    data_squeezed=False,\n",
    "    input_towards_future=False, output_towards_future=False, \n",
    "    output_include_current_timestep=True,\n",
    "    use_filtered_data=False, #lpcutoff=CUTOFF, lporder=FILT_ORDER, lpsamplfreq=SAMPL_FREQ,\n",
    "    return_data_arrays_orig=False,\n",
    "    return_data_arrays_processed=False,\n",
    "    return_train_val_test_arrays=False,\n",
    "    return_train_val_test_data=True,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# Correct the output feature count (this is buggy behavior and should be fixed)\n",
    "hparams['out_features'] = 8\n",
    "\n",
    "print(\"Extracting downsampled input and output data from the datacell ...\")\n",
    "# Inputs MUST have correct shape\n",
    "x_train = data_processed[\"x_train\"][::ds]\n",
    "x_val = data_processed[\"x_val\"][::ds]\n",
    "x_test = data_processed[\"x_test\"][::ds]\n",
    "# Outputs MUST be zero-indexed class labels\n",
    "y_train = data_processed[\"y_train\"][::ds]\n",
    "y_val = data_processed[\"y_val\"][::ds]\n",
    "y_test = data_processed[\"y_test\"][::ds]\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_val shape: \", x_val.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_val shape: \", y_val.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "del data_processed\n",
    "# Make datasets from training, validation and test sets\n",
    "print(\"Generating the TensorDataset objects ...\")\n",
    "train_set = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "val_set = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "test_set = TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).long())\n",
    "\n",
    "# If it is the first iteration of the loop, save the hyperparameters dictionary in the k-fold study dictionary\n",
    "k_fold_study['hparams'] = hparams\n",
    "\n",
    "# Construct model\n",
    "print(\"Constructing the model ...\")\n",
    "hparams['input_shape'] = list(x_train.shape[1:])\n",
    "hparams['output_shape'] = [8]\n",
    "print(\"Model input shape: \", hparams['input_shape'])\n",
    "print(\"Model output shape: \", hparams['output_shape'])\n",
    "model = EMGNet(hparams)\n",
    "print(model)\n",
    "\n",
    "# Train model\n",
    "print(\"Training the model ...\")\n",
    "# history = train_pytorch_model(\n",
    "#     model, [train_set, val_set], batch_size=1024, loss_str='crossentropy', optimizer_str='adam', \n",
    "#     optimizer_params={'weight_decay':0.0001}, loss_function_params=None, learnrate=0.1, \n",
    "#     learnrate_decay_gamma=0.95, epochs=200, validation_patience=1000000, \n",
    "#     verbose=1, script_before_save=True, saveto=None, num_workers=0)\n",
    "history = model.train_model([train_set, val_set], verbose=1)    \n",
    "\n",
    "# Update relevant fields in the k-fold study dictionary\n",
    "print(\"Updating the dictinoary for logging ...\")\n",
    "k_fold_study['history_training_loss'].append(history[\"training_loss\"])\n",
    "k_fold_study[\"history_validation_loss\"].append(history[\"validation_loss\"])\n",
    "k_fold_study[\"history_training_metrics\"].append(history[\"training_metrics\"])\n",
    "k_fold_study[\"history_validation_metrics\"].append(history[\"validation_metrics\"])\n",
    "k_fold_study[\"training_accuracies\"].append(history[\"training_metrics\"][-1])\n",
    "k_fold_study[\"validation_accuracies\"].append(history[\"validation_metrics\"][-1])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating the model on the test set ...\")\n",
    "# results = evaluate_pytorch_model(model, test_set, loss_str='crossentropy', loss_function_params=None,\n",
    "# batch_size=1024, device_str=\"cuda\", verbose=True, num_workers=0)\n",
    "results = model.evaluate_model(test_set, verbose=True)\n",
    "print(\"test set accuracy before TL: \")\n",
    "print(results[\"metrics\"])\n",
    "k_fold_study[\"testset_accuracies\"].append(results[\"metrics\"])\n",
    "torch.save(model, make_path(\"../models/\"+hparams['model_name']+\"/capg_dba_v004_PRE_MODEL.pt\"))\n",
    "print(\"Done with the pre-training.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dumping the JSON file ...\")\n",
    "json.dump(k_fold_study, open(make_path(\"../results/\"+hparams['model_name']+\"/k_fold_study.json\"), \"w\"), indent=4)\n",
    "print(\"Saved the JSON file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving general statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving the general statistics ...\")\n",
    "trn_acc_arr = np.array(k_fold_study[\"training_accuracies\"])\n",
    "val_acc_arr = np.array(k_fold_study[\"validation_accuracies\"])\n",
    "tst_acc_arr = np.array(k_fold_study[\"testset_accuracies\"])\n",
    "general_dict = {\"training_accuracy\":trn_acc_arr, \"validation_accuracy\":val_acc_arr, \"testset_accuracy\":tst_acc_arr}\n",
    "general_results = pd.DataFrame(general_dict)\n",
    "print(\"Description of general results:\")\n",
    "general_results_describe = general_results.describe()\n",
    "display(general_results_describe)\n",
    "general_results_describe.to_csv(\n",
    "    make_path(\"../results/\"+hparams['model_name']+\"/general_results.csv\"), header=True, index=True)\n",
    "print(\"Saved general statistics.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import json\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_fold_study = json.load(open(\"../results/capg_replica_dba_v002_2023_01_07_20_07_25/k_fold_study.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting the taining curve ...\")\n",
    "train_loss = np.array(k_fold_study[\"history_training_loss\"])\n",
    "val_loss = np.array(k_fold_study[\"history_validation_loss\"])\n",
    "train_acc = np.array(k_fold_study[\"history_training_metrics\"])\n",
    "val_acc = np.array(k_fold_study[\"history_validation_metrics\"])\n",
    "\n",
    "print(\"Shape of train_loss: \", train_loss.shape)\n",
    "\n",
    "train_loss_mean = np.mean(train_loss, axis=0)\n",
    "train_loss_std = np.std(train_loss, axis=0)# / 2\n",
    "val_loss_mean = np.mean(val_loss, axis=0)\n",
    "val_loss_std = np.std(val_loss, axis=0)# / 2\n",
    "train_acc_mean = np.mean(train_acc, axis=0)\n",
    "train_acc_std = np.std(train_acc, axis=0)# / 2\n",
    "val_acc_mean = np.mean(val_acc, axis=0)\n",
    "val_acc_std = np.std(val_acc, axis=0)# / 2\n",
    "\n",
    "print(\"Shape of train_loss_mean: \", train_loss_mean.shape)\n",
    "print(\"Shape of train_loss_std: \", train_loss_std.shape)\n",
    "\n",
    "epochs = train_loss_mean.shape[0]\n",
    "epochs = np.arange(1, epochs+1)\n",
    "plt.figure(figsize=(8,8), dpi=100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.grid(True)\n",
    "plt.plot(epochs, train_loss_mean, label=\"Training\", color=\"blue\")\n",
    "plt.fill_between(epochs, train_loss_mean-train_loss_std, train_loss_mean+train_loss_std, \n",
    "                 color='blue', alpha=0.2)\n",
    "plt.plot(epochs, val_loss_mean, label=\"Validation\", color=\"orange\")\n",
    "plt.fill_between(epochs, val_loss_mean-val_loss_std, val_loss_mean+val_loss_std,\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.grid(True)\n",
    "plt.plot(epochs, train_acc_mean, color=\"blue\")\n",
    "plt.fill_between(epochs, train_acc_mean-train_acc_std, train_acc_mean+train_acc_std,\n",
    "                 color='blue', alpha=0.2)\n",
    "plt.plot(epochs, val_acc_mean, color=\"orange\")\n",
    "plt.fill_between(epochs, val_acc_mean-val_acc_std, val_acc_mean+val_acc_std,\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.savefig(make_path(\"../results/\"+k_fold_study['hparams']['model_name']+\"/training_history.png\"), dpi=300)\n",
    "\n",
    "print(\"Done plotting the training curve.\")\n",
    "print(\"ALL DONE. GOOD BYE!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking out the BN parameters before TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_paras_mean = []\n",
    "bn_paras_var = []\n",
    "for layer in model.modules():\n",
    "   if isinstance(layer,(torch.nn.modules.batchnorm.BatchNorm3d,torch.nn.modules.batchnorm.BatchNorm1d)): \n",
    "       bn_paras_mean.append(layer.running_mean.cpu().numpy())\n",
    "       bn_paras_var.append(layer.running_var.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the TL part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the non-BatchNorm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = EMGNet(hparams)\n",
    "# x = torch.ones((32,1,64,8,16),dtype=torch.float32, requires_grad=False)*100\n",
    "# y = model_2(x)\n",
    "# # model_2.train()\n",
    "# # model_2.cpu()\n",
    "\n",
    "# for module in model_2.modules():\n",
    "#     if isinstance(module,(nn.BatchNorm3d,nn.BatchNorm1d)):\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(module.running_mean)\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         # y = model_2(torch.ones((32,1,64,8,16),dtype=torch.float32))\n",
    "#         # print(module.running_mean)\n",
    "#         break\n",
    "\n",
    "# for module in model_2.modules():\n",
    "#     #print(\"Next module \\n\")\n",
    "    \n",
    "#     if not isinstance(module, (torch.nn.modules.batchnorm.BatchNorm3d,torch.nn.modules.batchnorm.BatchNorm1d)):\n",
    "#         for param in module.parameters():\n",
    "#             param.requires_grad_(False)\n",
    "#         #module.eval()\n",
    "#     else:\n",
    "#         module.reset_running_stats()\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "# x = torch.ones((32,1,64,8,16),dtype=torch.float32,requires_grad=False)*10000\n",
    "# y = model_2(x)\n",
    "# # model_2.train()\n",
    "# # model_2.cpu()\n",
    "# for module in model_2.modules():\n",
    "#     if isinstance(module,(nn.BatchNorm3d,nn.BatchNorm1d)):\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(module.running_mean)\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         # y = model_2(torch.ones((32,1,64,8,16),dtype=torch.float32))\n",
    "#         # print(module.running_mean)\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for module in model.modules():\n",
    "    #print(\"Next module \\n\")\n",
    "    \n",
    "    if not isinstance(module, (torch.nn.modules.batchnorm.BatchNorm3d,torch.nn.modules.batchnorm.BatchNorm1d)):\n",
    "        # for param in module.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # #module.eval()\n",
    "        pass\n",
    "    else:\n",
    "        module.reset_running_stats()\n",
    "        \n",
    "\n",
    "# for module in model.modules():\n",
    "#     if isinstance(module,(nn.BatchNorm3d,nn.BatchNorm1d)):\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(module.running_mean)\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         print(\"---------------------------------------------------\\n\")\n",
    "#         y = module(torch.ones((32,1,64,8,16),dtype=torch.float32))\n",
    "#         print(module.running_mean)\n",
    "#         break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the testing data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_TL = x_test[:len(x_test)//2]\n",
    "y_train_TL = y_test[:len(y_test)//2]\n",
    "\n",
    "train_set_TL = TensorDataset(torch.from_numpy(x_train_TL).float(), torch.from_numpy(y_train_TL).long())\n",
    "\n",
    "\n",
    "x_val_TL = x_train_TL[:len(x_train_TL)//10]\n",
    "y_val_TL = y_train_TL[:len(y_train_TL)//10]\n",
    "\n",
    "val_set_TL = TensorDataset(torch.from_numpy(x_val_TL).float(), torch.from_numpy(y_val_TL).long())\n",
    "\n",
    "\n",
    "x_test_TL = x_test[len(x_test)//2:]\n",
    "y_test_TL = y_test[len(y_test)//2:]\n",
    "\n",
    "test_set_TL = TensorDataset(torch.from_numpy(x_test_TL).float(), torch.from_numpy(y_test_TL).long())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Functions for TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_metrics_for_batch(\n",
    "    predictions:torch.Tensor, targets:torch.Tensor, loss_str:str, classification:bool, regression:bool, \n",
    "    verbose:int, batch_num:int, epoch:int, metric:float, num_logits:int):\n",
    "    if loss_str == \"BCELoss\":\n",
    "        # Output layer already includes sigmoid.\n",
    "        class_predictions = (predictions > 0.5).int()\n",
    "    elif loss_str == \"BCEWithLogitsLoss\":\n",
    "        # Output layer does not include sigmoid. Sigmoid is a part of the loss function.\n",
    "        class_predictions = (torch.sigmoid(predictions) > 0.5).int()\n",
    "    elif loss_str in [\"NLLLoss\", \"CrossEntropyLoss\"]:\n",
    "        # nll -> Output layer already includes log_softmax.\n",
    "        # crossentropy -> Output layer has no log_softmax. It's implemented as a part of the loss function.\n",
    "        class_predictions = torch.argmax(predictions, dim=1)\n",
    "        if predictions.shape == targets.shape: # Targets are one-hot encoded probabilities\n",
    "            target_predictions = torch.argmax(targets, dim=1)\n",
    "        else: # Targets are class indices\n",
    "            target_predictions = targets\n",
    "\n",
    "    if classification:\n",
    "        if verbose>=2 and batch_num==0 and epoch ==0: \n",
    "            print(\"Shape of model outputs:     \", predictions.shape)\n",
    "            print(\"Shape of class predictions: \", class_predictions.shape)\n",
    "            print(\"Shape of targets:           \", targets.shape)\n",
    "        # Calculate accuracy\n",
    "        correct = (class_predictions == target_predictions).int().sum().item()\n",
    "        num_logits += target_predictions.numel()\n",
    "        metric += correct\n",
    "        if verbose==3 and epoch==0: \n",
    "            print(\"Number of correct answers (this batch - total): %10d - %10d\"%(correct, metric))\n",
    "        # Calculate F1 score\n",
    "        # f1 = f1_score(targets.cpu().numpy(), class_predictions.cpu().numpy(), average=\"macro\")\n",
    "    elif regression:\n",
    "        if verbose==3 and batch_num==0 and epoch==0: \n",
    "            print(\"Shape of predictions: \", predictions.shape)\n",
    "            print(\"Shape of targets:     \", targets.shape)\n",
    "        # Calculate r2_score\n",
    "        metric += r2_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    return metric, num_logits\n",
    "\n",
    "\n",
    "\n",
    "def _test_shapes(predictions:torch.Tensor, targets:torch.Tensor, classification:bool):\n",
    "    if classification:\n",
    "        assert predictions.shape[0] == targets.shape[0], \"Batch size of targets and predictions must be the same.\\n\"+\\\n",
    "            \"Target shape: %s, Prediction shape: %s\\n\"%(str(targets.shape), str(predictions.shape))\n",
    "        if len(predictions.shape) == 1:\n",
    "            assert targets.shape == predictions.shape, \"For 1D predictions, the targets must also be 1D.\\n\"+\\\n",
    "                \"Predictions shape: %s, Targets shape: %s\\n\"%(str(predictions.shape), str(targets.shape))\n",
    "        if len(predictions.shape) == 2:\n",
    "            assert len(targets.shape)==1 or targets.shape == predictions.shape, \\\n",
    "                \"For 2D predictions, the targets must be 1D class indices are 2D [N x K] one-hot encoded array, with the same shape as the predictions.\\n\"+\\\n",
    "                \"Predictions shape: %s, Targets shape: %s\\n\"%(str(predictions.shape), str(targets.shape))\n",
    "        if len(predictions.shape) > 2:\n",
    "            assert len(predictions.shape)==len(targets.shape) or len(predictions.shape)==len(targets.shape)+1, \\\n",
    "                \"Target dimensionality must be equal to or one less than the prediction dimensionality.\\n\"+\\\n",
    "                \"Target shape: %s, Prediction shape: %s\\n\"%(str(targets.shape), str(predictions.shape))+\\\n",
    "                \"If targets are class indices, they must be of shape (N,), or (N, d1, ..., dm). \"+\\\n",
    "                \"Otherwise, they must be (N, K) or (N, K, d1, ..., dm) arrays of one-hot encoded probabilities. \"+\\\n",
    "                \"Predictions must in any case be (N, K) or (N, K, d1, ..., dm).\\n\"+\\\n",
    "                \"N is batch size, K is number of classes and d1 to dm are other dimensionalities of classification, if any.\"\n",
    "            if len(predictions.shape) == len(targets.shape):\n",
    "                assert predictions.shape == targets.shape, \"If predictions and targets have the same dimensionality, they must be the same shape.\\n\"+\\\n",
    "                    \"Target shape: %s, Prediction shape: %s\\n\"%(str(targets.shape), str(predictions.shape))\n",
    "            else:\n",
    "                assert predictions.shape[2:] == targets.shape[1:], \\\n",
    "                    \"If predictions have shape (N, K, d1, ..., dm) then targets must either have the same shape, or (N, d1, ..., dm).\\n\"+\\\n",
    "                    \"Target shape: %s, Prediction shape: %s\\n\"%(str(targets.shape), str(predictions.shape))\n",
    "    else:\n",
    "        assert predictions.shape == targets.shape, \\\n",
    "            \"Target shape must be equal to the prediction shape.\\n\"+\\\n",
    "            \"Target shape: %s, Prediction shape: %s\\n\"%(str(targets.shape), str(predictions.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _calculate_epoch_loss_and_metrics(\n",
    "    cumulative_epoch_loss:float, num_batches:int, verbose:int, epoch:int, \n",
    "    hist_loss:dict, hist_metric:dict, display_metrics:bool, cumulative_metric:float, metric_denominator:int):\n",
    "    # Calculate training epoch loss\n",
    "    epoch_loss = cumulative_epoch_loss / num_batches\n",
    "    if verbose==3 and epoch==0: print(\"Epoch loss (training): %.5f\"%epoch_loss)\n",
    "    if hist_loss is not None: hist_loss.append(epoch_loss)\n",
    "    # Calculate training epoch metric (accuracy or r2-score)\n",
    "    if display_metrics:\n",
    "        epoch_metric = cumulative_metric / metric_denominator\n",
    "        if verbose==3 and epoch==0: print(\"Epoch metric: %.5f\"%epoch_metric)\n",
    "        if hist_metric is not None: hist_metric.append(epoch_metric)\n",
    "    return epoch_loss, epoch_metric, hist_loss, hist_metric\n",
    "\n",
    "\n",
    "\n",
    "def save_pytorch_model(model:torch.nn.Module, saveto:str, dataloader, script_before_save:bool=True, verbose:int=1):\n",
    "    try:\n",
    "        if verbose > 0: print(\"Saving model...\")\n",
    "        if script_before_save:\n",
    "            example,_ = next(iter(dataloader))\n",
    "            example = example[0,:].unsqueeze(0)\n",
    "            model.cpu()\n",
    "            with torch.no_grad():\n",
    "                traced = torch.jit.trace(model, example)\n",
    "                traced.save(saveto)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                torch.save(model, saveto)\n",
    "    except Exception as e:\n",
    "        if verbose > 0:\n",
    "            print(e)\n",
    "            print(\"Failed to save the model.\")\n",
    "    if verbose > 0: print(\"Done Saving.\")\n",
    "    \n",
    "\n",
    "\n",
    "def train_pytorch_model_TL(model, dataset, batch_size:int, loss_str:str, optimizer_str:str, optimizer_params:dict=None, loss_function_params:dict=None, learnrate:float=0.001, \n",
    "    learnrate_decay_gamma:float=None, epochs:int=10, validation_patience:int=10000, validation_data:float=0.1, verbose:int=1, script_before_save:bool=True, saveto:str=None, \n",
    "    num_workers=0):\n",
    "    \"\"\"Train a Pytorch model, given some hyperparameters.\n",
    "\n",
    "    ### Args:\n",
    "        - `model` (`torch.nn`): A torch.nn model\n",
    "        - `dataset` (`torch.utils.data.Dataset`): Dataset object to be used\n",
    "        - `batch_size` (int): Batch size\n",
    "        - `loss_str` (str): Loss function to be used. Examples: \"CrossEntropyLoss\", \"BCELoss\", \"BCEWithLogitsLoss\", \"MSELoss\", etc.\n",
    "        - `optimizer_str` (str): Optimizer to be used. Examples: \"Adam\", \"SGD\", \"RMSprop\", etc.\n",
    "        - `optimizer_params` (dict, optional): Parameters for the optimizer.\n",
    "        - `loss_function_params` (dict, optional): Parameters for the loss function.\n",
    "        - `learnrate` (float, optional): Learning rate. Defaults to 0.001.\n",
    "        - `learnrate_decay_gamma` (float, optional): Learning rate exponential decay rate. Defaults to None.\n",
    "        - `epochs` (int, optional): Number of epochs. Defaults to 10.\n",
    "        - `validation_patience` (int, optional): Number of epochs to wait before stopping training. Defaults to 10000.\n",
    "        - `validation_data` (float, optional): Fraction of the dataset to be used for validation. Defaults to 0.1.\n",
    "        - `verbose` (int, optional): Logging the progress. Defaults to 1. 0 prints nothing, 2 prints everything.\n",
    "        - `script_before_save` (bool, optional): Use TorchScript for serializing the model. Defaults to True.\n",
    "        - `saveto` (str, optional): Save PyTorch model in path. Defaults to None.\n",
    "        - `num_workers` (int, optional): Number of workers for the dataloader. Defaults to 0.\n",
    "        \n",
    "    ### Returns:\n",
    "        - `model`: Trained PyTorch-compatible model\n",
    "        - `history`: PyTorch model history dictionary, containing the following keys:\n",
    "            - `training_loss`: List containing training loss values of epochs.\n",
    "            - `validation_loss`: List containing validation loss values of epochs.\n",
    "            - `learning_rate`: List containing learning rate values of epochs.\n",
    "            - `training_metrics`: List containing training metric values of epochs.\n",
    "            - `validation_metrics`: List containing validation metric values of epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, random_split, Dataset, TensorDataset\n",
    "    SEED = 42\n",
    "    from timeit import default_timer as timer\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    \n",
    "    # Initialize necessary lists\n",
    "    hist_training_loss = []\n",
    "    hist_validation_loss = []\n",
    "    hist_learning_rate = []\n",
    "    hist_trn_metric = []\n",
    "    hist_val_metric = []\n",
    "    \n",
    "    # Empty CUDA cache\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check if validation data is provided or not, and calculate number of training and validation data\n",
    "    if isinstance(dataset, (list, tuple)):\n",
    "        assert len(dataset)==2, \"If dataset is a tuple, it must have only two elements, the training dataset and the validation dataset.\"\n",
    "        trainset, valset = dataset\n",
    "        num_val_data = int(len(valset))\n",
    "        num_train_data = int(len(trainset))\n",
    "        num_all_data = num_train_data + num_val_data\n",
    "    else:\n",
    "        num_all_data = len(dataset)\n",
    "        num_val_data = int(validation_data*num_all_data)\n",
    "        num_train_data = num_all_data - num_val_data\n",
    "        (trainset, valset) = random_split(dataset, (num_train_data, num_val_data), generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"Total number of data points:      %d\"%num_all_data)\n",
    "        print(\"Number of training data points:   %d\"%num_train_data)\n",
    "        print(\"Number of validation data points: %d\"%num_val_data)\n",
    "    \n",
    "    # Generate training and validation dataloaders    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    validloader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(\"Number of training batches:    %d\"%len(trainloader))\n",
    "        print(\"Number of validation batches:  %d\"%len(validloader))\n",
    "        print(\"Batch size:                    %d\"%batch_size)\n",
    "        for x,y in trainloader:\n",
    "            print(\"Shape of training input from the dataloader:  \", x.shape)\n",
    "            print(\"Shape of training output from the dataloader: \", y.shape)\n",
    "            break\n",
    "        for x,y in validloader:\n",
    "            print(\"Shape of validation input from the dataloader:  \", x.shape)\n",
    "            print(\"Shape of validation output from the dataloader: \", y.shape)\n",
    "            break\n",
    "    \n",
    "    # Select the device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if verbose > 0: print(\"Selected device: \", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Instantiate the loss function\n",
    "    loss_func = getattr(nn, loss_str)\n",
    "    criterion = loss_func(**loss_function_params) if loss_function_params else loss_func()\n",
    "        \n",
    "    # Instantiate the optimizer\n",
    "    optimizer_func = getattr(optim, optimizer_str)\n",
    "    optimizer = optimizer_func(model.parameters(), lr=learnrate, **optimizer_params) if optimizer_params else optimizer_func(model.parameters(), lr=learnrate)\n",
    "    \n",
    "    # Defining learning rate scheduling\n",
    "    if learnrate_decay_gamma:\n",
    "        if verbose > 0: print(\"The learning rate has an exponential decay rate of %.5f.\"%learnrate_decay_gamma)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=learnrate_decay_gamma)\n",
    "        lr_sch = True\n",
    "    else:\n",
    "        lr_sch = False\n",
    "    \n",
    "    # Find out if we will display any metric along with the loss.\n",
    "    display_metrics = True\n",
    "    classification = True\n",
    "    regression = False\n",
    "    if loss_str in [\"BCELoss\", \"BCEWithLogitsLoss\", \"CrossEntropyLoss\", \"NLLLoss\", \"PoissonNLLLoss\", \"GaussianNLLLoss\"]:\n",
    "        classification = True\n",
    "        regression = False\n",
    "        trn_metric_name = \"Acc\"\n",
    "        val_metric_name = \"Val Acc\"\n",
    "    elif loss_str in [\"MSELoss\", \"L1Loss\", \"L2Loss\", \"HuberLoss\", \"SmoothL1Loss\"]:\n",
    "        classification = False\n",
    "        regression = True\n",
    "        trn_metric_name = \"R2\"\n",
    "        val_metric_name = \"Val R2\"\n",
    "    else:\n",
    "        classification = False\n",
    "        regression = False\n",
    "        display_metrics = False\n",
    "    if verbose > 0:\n",
    "        if classification: print(\"Classification problem detected. We will look at accuracies.\")\n",
    "        elif regression: print(\"Regression problem detected. We will look at R2 scores.\")\n",
    "        else: print(\"We have detected neither classification nor regression problem. No metric will be displayed other than loss.\")\n",
    "    \n",
    "                    \n",
    "    # Calculating number of training and validation batches\n",
    "    num_training_batches = len(trainloader)\n",
    "    num_validation_batches = len(validloader)\n",
    "    \n",
    "    # Preparing progress bar\n",
    "    progress_bar_size = 40\n",
    "    ch = \"█\"\n",
    "    intvl = num_training_batches/progress_bar_size;\n",
    "    valtol = validation_patience if validation_patience else 100000000\n",
    "    minvalerr = 10000000000.0\n",
    "    badvalcount = 0\n",
    "    \n",
    "    # Commencing training loop\n",
    "    tStart = timer()\n",
    "    loop = tqdm(range(epochs), desc='Training Progress', ncols=100) if verbose==1 else range(epochs)\n",
    "    for epoch in loop:\n",
    "        \n",
    "        # Initialize per-epoch variables\n",
    "        tEpochStart = timer()\n",
    "        epoch_loss_training = 0.0\n",
    "        epoch_loss_validation = 0.0\n",
    "        newnum = 0\n",
    "        oldnum = 0\n",
    "        trn_metric = 0.0\n",
    "        val_metric = 0.0\n",
    "        num_train_logits = 0\n",
    "        num_val_logits = 0\n",
    "    \n",
    "        if verbose>=2 and epoch > 0: print(\"Epoch %3d/%3d [\"%(epoch+1, epochs), end=\"\")\n",
    "        if verbose==3 and epoch ==0: print(\"First epoch ...\")\n",
    "        \n",
    "        ##########################################################################\n",
    "        # Training\n",
    "        if verbose==3 and epoch==0: print(\"\\nTraining phase ...\")\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # Fetch data\n",
    "            seqs, targets = data[0].to(device), data[1].to(device)\n",
    "            # Forward propagation\n",
    "            predictions = model(seqs)\n",
    "            # for module in model.modules():\n",
    "            #     if isinstance(module,(nn.BatchNorm3d,nn.BatchNorm1d)):\n",
    "            #         print(\"---------------------------------------------------\\n\")\n",
    "            #         print(\"---------------------------------------------------\\n\")\n",
    "            #         print(module.running_mean)\n",
    "            #         print(\"---------------------------------------------------\\n\")\n",
    "            #         print(\"---------------------------------------------------\\n\")\n",
    "            #         break\n",
    "            # Test shapes\n",
    "            _test_shapes(predictions, targets, classification)\n",
    "            # Loss calculation and accumulation\n",
    "            loss = criterion(predictions, targets)\n",
    "            epoch_loss_training += loss.item()\n",
    "            # Backpropagation and optimizer update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics calculation\n",
    "            if display_metrics:\n",
    "                with torch.no_grad():\n",
    "                    trn_metric, num_train_logits = _update_metrics_for_batch(\n",
    "                        predictions, targets, loss_str, classification, regression, verbose, i, epoch, trn_metric, num_train_logits)\n",
    "                    \n",
    "            # Visualization of progressbar within the batch\n",
    "            if verbose>=2 and epoch > 0:\n",
    "                newnum = int(i/intvl)\n",
    "                if newnum > oldnum:\n",
    "                    print((newnum-oldnum)*ch, end=\"\")\n",
    "                    oldnum = newnum \n",
    "        \n",
    "        # Update learning rate if necessary\n",
    "        if lr_sch: scheduler.step()\n",
    "        \n",
    "        # Calculate epoch loss and metrics\n",
    "        epoch_loss_training, trn_metric, hist_training_loss, hist_trn_metric = _calculate_epoch_loss_and_metrics(epoch_loss_training, num_training_batches, verbose, epoch, \n",
    "            hist_training_loss, hist_trn_metric, display_metrics, trn_metric, (num_train_logits if classification else num_training_batches))\n",
    "            \n",
    "        if verbose>=2 and epoch > 0: print(\"] \", end=\"\")\n",
    "        \n",
    "        # ##########################################################################\n",
    "        # # Validation\n",
    "        # if verbose==3 and epoch==0: print(\"\\nValidation phase ...\")\n",
    "        # # model.eval()\n",
    "        # with torch.no_grad():\n",
    "        #     for i, data in enumerate(validloader):\n",
    "        #         seqs, targets = data[0].to(device), data[1].to(device)\n",
    "        #         predictions = model(seqs)\n",
    "        #         loss = criterion(predictions, targets)\n",
    "        #         epoch_loss_validation += loss.item()\n",
    "        #         # Do prediction for metrics\n",
    "        #         if display_metrics:\n",
    "        #             val_metric, num_val_logits = _update_metrics_for_batch(\n",
    "        #                 predictions, targets, loss_str, classification, regression, verbose, i, epoch, val_metric, num_val_logits)\n",
    "        # # Calculate epoch loss and metrics\n",
    "        # epoch_loss_validation, val_metric, hist_validation_loss, hist_val_metric = _calculate_epoch_loss_and_metrics(epoch_loss_validation, num_validation_batches, verbose, epoch, \n",
    "        #     hist_validation_loss, hist_val_metric, display_metrics, val_metric, (num_val_logits if classification else num_validation_batches))\n",
    "        \n",
    "        # Log the learning rate, if there is any scheduling.\n",
    "        # if lr_sch: hist_learning_rate.append(scheduler.get_last_lr()[0])\n",
    "        # else: hist_learning_rate.append(learnrate)\n",
    "        \n",
    "        ##########################################################################\n",
    "        # Post Processing Training Loop            \n",
    "        tEpochEnd = timer()\n",
    "        if verbose>=2:\n",
    "            if display_metrics:\n",
    "                print(\"Loss: %5.4f |%s: %5.4f \" % (\n",
    "                    epoch_loss_training, trn_metric_name, trn_metric))\n",
    "            else:\n",
    "                print(\"Loss: %5.4f\" % (epoch_loss_training))\n",
    "        \n",
    "        # # Checking for early stopping\n",
    "        # if epoch_loss_validation < minvalerr:\n",
    "        #     minvalerr = epoch_loss_validation\n",
    "        #     badvalcount = 0\n",
    "        # else:\n",
    "        #     badvalcount += 1\n",
    "        #     if badvalcount > valtol:\n",
    "        #         if verbose > 0:\n",
    "        #             print(\"Validation loss not improved for more than %d epochs.\"%badvalcount)\n",
    "        #             print(\"Early stopping criterion with validation loss has been reached. \" + \n",
    "        #                 \"Stopping training at %d epochs...\"%epoch)\n",
    "    #     #         break\n",
    "    # # End for loop\n",
    "    # # model.eval()\n",
    "    # ##########################################################################\n",
    "    # # Epilogue\n",
    "    # tFinish = timer()\n",
    "    # if verbose > 0:        \n",
    "    #     print('Finished Training.')\n",
    "    #     print(\"Training process took %.2f seconds.\"%(tFinish-tStart))\n",
    "    # # if saveto:\n",
    "    # #    save_pytorch_model(model, saveto, trainloader, script_before_save, verbose)\n",
    "    # # Clear CUDA cache    \n",
    "    # if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    # # Generate output dictionaries\n",
    "    # history = {\n",
    "    #     'training_loss':hist_training_loss}\n",
    "    # if display_metrics:\n",
    "    #     history[\"training_metrics\"] = hist_trn_metric\n",
    "    # if verbose > 0: print(\"Done training.\")\n",
    "    \n",
    "    #return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pytorch_model_TL(model, [train_set_TL,val_set_TL], batch_size=hparams[\"batch_size\"], loss_str=hparams[\"loss_function\"], optimizer_str=hparams[\"optimizer\"], \n",
    "    optimizer_params=None, loss_function_params=None, learnrate=hparams[\"learning_rate\"], \n",
    "    learnrate_decay_gamma=hparams[\"learning_rate_decay_gamma\"], epochs=1, validation_patience=10000, validation_data=0.1, verbose=1, script_before_save=False, saveto=None, \n",
    "    num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the model on the test set ...\")\n",
    "# results = evaluate_pytorch_model(model, test_set, loss_str='crossentropy', loss_function_params=None,\n",
    "# batch_size=1024, device_str=\"cuda\", verbose=True, num_workers=0)\n",
    "results = model.evaluate_model(test_set_TL, verbose=True)\n",
    "print(\"test set accuracy after TL: \")\n",
    "print(results[\"metrics\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking out the BN parameters after TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_paras_mean_TL = []\n",
    "bn_paras_var_TL = []\n",
    "for layer in model.modules():\n",
    "   if isinstance(layer,(torch.nn.modules.batchnorm.BatchNorm3d,torch.nn.modules.batchnorm.BatchNorm1d)): \n",
    "       bn_paras_mean_TL.append(layer.running_mean.cpu().numpy())\n",
    "       bn_paras_var_TL.append(layer.running_var.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bn_paras_mean == bn_paras_mean_TL)\n",
    "print(bn_paras_var == bn_paras_var_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(bn_paras_mean[idx])\n",
    "print(bn_paras_mean_TL[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(bn_paras_mean[idx])\n",
    "print(bn_paras_mean_TL[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76c97409fe1b9249810273818a125b7bb9a089b64605bbece0f2179e5c237ad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
