{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition with CAPG DB-a Dataset Using 3D CNN with EMGNet Architecture (one subject for testing)\n",
    "\n",
    "In this preliminary effort, we will try to perform hand gesture recognition on CAPG DBA dataset.\n",
    "We will use the EMGNet architecture and training procedure, but instead of CWT, we will use 3D CNN on sequences of 2D images.\n",
    "\n",
    "In this version:\n",
    "\n",
    "- EMG data is normalized with the recorded MVC data\n",
    "- The **EMGNet** architecture will be used, along with the training procedure.\n",
    "- A **3D CNN** architecture will be adopted into the EMGNet architecture.\n",
    "- **Raw EMG data** will be used, there will be no preproccessing or feature engineering.\n",
    "- **Training data:** 17 subjects\n",
    "- **Test data:** 1 subject\n",
    "- K-fold cross-validation will be performed.\n",
    "\n",
    "**NOTE** This code has been tested with:\n",
    "```\n",
    "    numpy version:        1.23.5\n",
    "    scipy version:        1.9.3\n",
    "    sklearn version:      1.2.0\n",
    "    seaborn version:      0.12.1\n",
    "    pandas version:       1.5.2\n",
    "    torch version:        1.12.1+cu113\n",
    "    matplotlib version:   3.6.2\n",
    "    CUDA version:         11.2\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Preliminaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in ./.local/lib/python3.8/site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./.local/lib/python3.8/site-packages (from pyarrow) (1.24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fastparquet in ./.conda/envs/Myenv/lib/python3.8/site-packages (2023.1.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in ./.local/lib/python3.8/site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in ./.conda/envs/Myenv/lib/python3.8/site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.local/lib/python3.8/site-packages (from fastparquet) (1.24.1)\n",
      "Requirement already satisfied: fsspec in ./.conda/envs/Myenv/lib/python3.8/site-packages (from fastparquet) (2023.1.0)\n",
      "Requirement already satisfied: packaging in ./.conda/envs/Myenv/lib/python3.8/site-packages (from fastparquet) (22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/Myenv/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.conda/envs/Myenv/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/Myenv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in ./.conda/envs/Myenv/lib/python3.8/site-packages (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.local/lib/python3.8/site-packages (from PyWavelets) (1.24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory is:  /scratch/users/edincer22\n",
      "Current Working Directory is now:  /scratch/users/edincer22/comp541-project/capg_3dcnn\n",
      "numpy version:        1.24.1\n",
      "scipy version:        1.9.3\n",
      "sklearn version:      1.2.0\n",
      "seaborn version:      0.12.2\n",
      "pandas version:       1.5.3\n",
      "torch version:        1.12.1+cu113\n",
      "matplotlib version:   3.6.2\n",
      "Checking to see if PyTorch recognizes GPU...\n",
      "True\n",
      "Using matplotlib backend: <object object at 0x2b09bdebd0b0>\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "direc = os.getcwd()\n",
    "print(\"Current Working Directory is: \", direc)\n",
    "KUACC = False\n",
    "if \"scratch\" in direc: # We are using the cluster\n",
    "    KUACC = True\n",
    "    homedir = os.path.expanduser(\"~\")\n",
    "    os.chdir(os.path.join(homedir,\"comp541-project/capg_3dcnn/\"))\n",
    "    direc = os.getcwd()\n",
    "    print(\"Current Working Directory is now: \", direc)\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../data/\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets_torch import *\n",
    "from models_torch import *\n",
    "from utils_torch import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import statistics\n",
    "import json\n",
    "from IPython.display import display\n",
    "#from cwt import calculate_wavelet_vector, calculate_wavelet_dataset\n",
    "\n",
    "# Print versions\n",
    "print(\"numpy version:       \", np.__version__)\n",
    "print(\"scipy version:       \", sp.__version__)\n",
    "print(\"sklearn version:     \", sklearn.__version__)\n",
    "print(\"seaborn version:     \", sns.__version__)\n",
    "print(\"pandas version:      \", pd.__version__)\n",
    "print(\"torch version:       \", torch.__version__)\n",
    "print(\"matplotlib version:  \", matplotlib.__version__)\n",
    "\n",
    "\n",
    "# Checking to see if CUDA is available for us\n",
    "print(\"Checking to see if PyTorch recognizes GPU...\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Whether to use latex rendering in plots throughout the notebook\n",
    "USE_TEX = False\n",
    "FONT_SIZE = 12\n",
    "\n",
    "# Setting matplotlib plotting variables\n",
    "if USE_TEX:\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.size\": FONT_SIZE,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Computer Modern Roman\"]\n",
    "    })\n",
    "else:\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.size\": FONT_SIZE,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\"]\n",
    "    })\n",
    "\n",
    "# Do not plot figures inline (only useful for cluster)\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Hyperparameters and Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General settings of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_study = {\n",
    "    'code':'capg_3dcnn/capg_dba_v003',\n",
    "    'package':'torch',\n",
    "    'dataset':'capg',\n",
    "    'subdataset':'dba',\n",
    "    \"training_accuracies\": [],\n",
    "    \"validation_accuracies\": [],\n",
    "    \"testset_accuracies\": [],\n",
    "    \"history_training_loss\": [],\n",
    "    \"history_training_metrics\": [],\n",
    "    \"history_validation_loss\": [],\n",
    "    \"history_validation_metrics\": [],\n",
    "    \"preprocessing\":None,\n",
    "    \"feature_engineering\":None,\n",
    "    \"k_fold_mode\":\"1 subject for testing\",\n",
    "    \"global_downsampling\":10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"model_name\": autoname(\"capg_3dcnn_dba_v003\"),\n",
    "    # General hyperparameters\n",
    "    \"in_features\": 128,\n",
    "    \"out_features\": 1,\n",
    "    # Sequence hyperparameters\n",
    "    \"in_seq_len_sec\": 0.16,\n",
    "    \"out_seq_len_sec\": 0,\n",
    "    \"data_sampling_rate_Hz\": 1000.0,\n",
    "    \"data_downsampling\": 5,\n",
    "    \"sequence_downsampling\": 1,\n",
    "    \"in_seq_len\": 0,\n",
    "    \"out_seq_len\": 0,\n",
    "    # Convolution blocks\n",
    "    \"num_conv_blocks\": 4,\n",
    "    \"conv_dim\": 3,\n",
    "    \"conv_params\": None,\n",
    "    \"conv_channels\": [16, 32, 32, 64],\n",
    "    \"conv_kernel_size\": 3,\n",
    "    \"conv_padding\": \"same\",\n",
    "    \"conv_stride\": 1,\n",
    "    \"conv_dilation\": 1,\n",
    "    \"conv_activation\": \"ReLU\",\n",
    "    \"conv_activation_params\": None,#{\"negative_slope\": 0.1},\n",
    "    \"conv_norm_layer_type\": \"BatchNorm\",\n",
    "    \"conv_norm_layer_position\": \"before\",\n",
    "    \"conv_norm_layer_params\": None,\n",
    "    \"conv_dropout\": None,\n",
    "    \"pool_type\": [None, None, None, \"AdaptiveAvg\"],\n",
    "    \"pool_kernel_size\": 2,\n",
    "    \"pool_padding\": 0,\n",
    "    \"pool_stride\": 1,\n",
    "    \"pool_dilation\": 1,\n",
    "    \"pool_params\": None,\n",
    "    \"min_image_size\": 1,\n",
    "    \"adaptive_pool_output_size\": [1,1,1],\n",
    "    # Fully connected blocks\n",
    "    \"dense_width\": \"auto\",\n",
    "    \"dense_depth\": 0,\n",
    "    \"dense_activation\": \"ReLU\",\n",
    "    \"dense_activation_params\": None,\n",
    "    \"output_activation\": None,\n",
    "    \"output_activation_params\": None,\n",
    "    \"dense_norm_layer_type\": None,\n",
    "    \"dense_norm_layer_position\": None,\n",
    "    \"dense_norm_layer_params\": None,\n",
    "    \"dense_dropout\": None,\n",
    "    # Training procedure\n",
    "    \"l2_reg\": 0.0001,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 40,\n",
    "    \"validation_data\": [0.05,'testset'],\n",
    "    \"validation_tolerance_epochs\": 1000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"learning_rate_decay_gamma\": 0.9,\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"optimizer_params\": None\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file:  dba_subj_18.parquet\n",
      "Loading file:  dba_subj_15.parquet\n",
      "Loading file:  dba_subj_7.parquet\n",
      "Loading file:  dba_subj_1.parquet\n",
      "Loading file:  dba_subj_16.parquet\n",
      "Loading file:  dba_subj_5.parquet\n",
      "Loading file:  dba_subj_13.parquet\n",
      "Loading file:  dba_subj_10.parquet\n",
      "Loading file:  dba_subj_6.parquet\n",
      "Loading file:  dba_subj_2.parquet\n",
      "Loading file:  dba_subj_12.parquet\n",
      "Loading file:  dba_subj_8.parquet\n",
      "Loading file:  dba_subj_4.parquet\n",
      "Loading file:  dba_subj_3.parquet\n",
      "Loading file:  dba_subj_14.parquet\n",
      "Loading file:  dba_subj_17.parquet\n",
      "Loading file:  dba_subj_9.parquet\n",
      "Loading file:  dba_subj_11.parquet\n",
      "dba_tot shape:  (1476000, 131)\n",
      "dba_mvc shape:  (36000, 131)\n",
      "dba shape:  (1440000, 131)\n",
      "Columns: \n",
      "Index(['subject', 'gesture', 'trial', 'b_1_c_1', 'b_1_c_2', 'b_1_c_3',\n",
      "       'b_1_c_4', 'b_1_c_5', 'b_1_c_6', 'b_1_c_7',\n",
      "       ...\n",
      "       'b_8_c_7', 'b_8_c_8', 'b_8_c_9', 'b_8_c_10', 'b_8_c_11', 'b_8_c_12',\n",
      "       'b_8_c_13', 'b_8_c_14', 'b_8_c_15', 'b_8_c_16'],\n",
      "      dtype='object', length=131)\n",
      "Description: \n",
      "            subject       gesture         trial\n",
      "count  1.440000e+06  1.440000e+06  1.440000e+06\n",
      "mean   9.500000e+00  4.500000e+00  5.500000e+00\n",
      "std    5.188129e+00  2.291289e+00  2.872282e+00\n",
      "min    1.000000e+00  1.000000e+00  1.000000e+00\n",
      "25%    5.000000e+00  2.750000e+00  3.000000e+00\n",
      "50%    9.500000e+00  4.500000e+00  5.500000e+00\n",
      "75%    1.400000e+01  6.250000e+00  8.000000e+00\n",
      "max    1.800000e+01  8.000000e+00  1.000000e+01\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/CAPG/parquet\"\n",
    "def load_single_capg_dataset(data_dir, db_str:str=\"dba\"):\n",
    "    data_lst = []\n",
    "    for i,file in enumerate(os.listdir(data_dir)):\n",
    "        if file.endswith(\".parquet\") and db_str in file:\n",
    "            print(\"Loading file: \", file)\n",
    "            data_lst.append(pd.read_parquet(os.path.join(data_dir, file)))\n",
    "    data = pd.concat(data_lst, axis=0, ignore_index=True)\n",
    "    return data\n",
    "dba_tot = load_single_capg_dataset(data_dir, db_str=\"dba\")\n",
    "dba_mvc = dba_tot.loc[dba_tot[\"gesture\"].isin([100, 101])]\n",
    "dba = dba_tot.loc[~dba_tot[\"gesture\"].isin([100, 101])]\n",
    "print(\"dba_tot shape: \", dba_tot.shape)\n",
    "print(\"dba_mvc shape: \", dba_mvc.shape)\n",
    "print(\"dba shape: \", dba.shape)\n",
    "print(\"Columns: \")\n",
    "print(dba_tot.columns)\n",
    "print(\"Description: \")\n",
    "print(dba.iloc[:,:3].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize EMG Data\n",
    "\n",
    "Here the recorded MVC values will be used for normalizaing EMG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121882/1353078018.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dba.iloc[:,3:] = dba.iloc[:,3:].div(max_mvc, axis=1)\n"
     ]
    }
   ],
   "source": [
    "max_mvc = dba_mvc.iloc[:,3:].max(axis=0)\n",
    "del dba_mvc\n",
    "# print(\"max_mvc for 5 first channels: \")\n",
    "# print(max_mvc[:5])\n",
    "# print(\"shape of max_mvc: \", max_mvc.shape)\n",
    "# print(\"max of dba before normalization: (first five)\")\n",
    "# print(dba.iloc[:,3:].max(axis=0)[:5])\n",
    "dba.iloc[:,3:] = dba.iloc[:,3:].div(max_mvc, axis=1)\n",
    "# print(\"max of dba_norm after normalization: \")\n",
    "# print(dba_norm.iloc[:,3:].max(axis=0)[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- k-fold study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMGNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMGNet(PyTorchSmartModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(EMGNet, self).__init__(hparams)\n",
    "        self.prep_block = nn.Sequential(\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main_block = Conv_Network(hparams)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep_block(x)\n",
    "        x = self.main_block(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform k-fold cross-validation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################################################################\n",
      "Using subject 1 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [1]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404197\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "EMGNet(\n",
      "  (prep_block): Sequential(\n",
      "    (0): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (main_block): Conv_Network(\n",
      "    (net): Sequential(\n",
      "      (0): Conv_Block(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
      "          (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): Conv_Block(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
      "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): Conv_Block(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
      "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (3): Conv_Block(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
      "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): AdaptiveAvgPool3d(output_size=[1, 1, 1])\n",
      "        )\n",
      "      )\n",
      "      (4): Flatten(start_dim=1, end_dim=-1)\n",
      "      (5): Linear(in_features=64, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:18<00:00, 67.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2718.48 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 4.4478 | Accuracy: 0.2664\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 2 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [2]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:17<00:00, 67.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2717.09 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 3.7788 | Accuracy: 0.3632\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 3 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [3]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:16<00:00, 67.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2716.49 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 2.9185 | Accuracy: 0.3664\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 4 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [4]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:18<00:00, 67.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2718.28 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 2.9183 | Accuracy: 0.4026\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 5 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [5]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:17<00:00, 67.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2717.29 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 3.4492 | Accuracy: 0.3737\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 6 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [6]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:17<00:00, 67.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2717.60 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 6.0269 | Accuracy: 0.2026\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 7 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [7]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:14<00:00, 67.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2714.44 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 5.6618 | Accuracy: 0.2651\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 8 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [8]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:16<00:00, 67.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2716.71 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 3.5937 | Accuracy: 0.2625\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 9 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [9]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:14<00:00, 67.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2714.07 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 1.8230 | Accuracy: 0.4974\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 10 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [10]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:14<00:00, 67.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2714.28 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 5.8002 | Accuracy: 0.1586\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 11 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [11]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:17<00:00, 67.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2717.61 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 10.8547 | Accuracy: 0.0993\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 12 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [12]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:18<00:00, 67.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2718.22 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 4.7330 | Accuracy: 0.2757\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 13 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [13]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:18<00:00, 67.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2718.02 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 4.3196 | Accuracy: 0.2461\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 14 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [14]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:17<00:00, 67.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2717.84 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 8.9482 | Accuracy: 0.1836\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 15 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [15]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404498\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████████| 40/40 [45:14<00:00, 67.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n",
      "Training process took 2714.35 seconds.\n",
      "Done training.\n",
      "Updating the dictinoary for logging ...\n",
      "Evaluating the model on the test set ...\n",
      "Preparing data...\n",
      "selected device:  cuda\n",
      "Evaluating model...\n",
      "[██████████████████████████] Loss: 1.9904 | Accuracy: 0.3691\n",
      "Done.\n",
      "Done with this fold of the K-fold study.\n",
      "\n",
      "#################################################################\n",
      "Using subject 16 for testing ...\n",
      "#################################################################\n",
      "\n",
      "Generating data cell ...\n",
      "# subjects:    18\n",
      "# conditions:  8\n",
      "# trials:      10\n",
      "\n",
      "\n",
      "subjects used for testing:    [16]\n",
      "conditions used for testing:  []\n",
      "trials used for testing:      []\n",
      "\n",
      "\n",
      "Iterating through all trials ...\n",
      "\n",
      "Concatenating arrays and generating outputs ...\n",
      "Validation data source:   testset\n",
      "Validation data portion:  0.05\n",
      "x_train:  (272000, 1, 64, 8, 16)\n",
      "y_train:  (272000,)\n",
      "x_val:  (800, 1, 64, 8, 16)\n",
      "y_val:  (800,)\n",
      "x_test:  (15200, 1, 64, 8, 16)\n",
      "y_test:  (15200,)\n",
      "Constructing output dictionary ...\n",
      "Size of output dictionary in bytes:  9438404470\n",
      "Done.\n",
      "\n",
      "Extracting downsampled input and output data from the datacell ...\n",
      "x_train shape:  (27200, 1, 64, 8, 16)\n",
      "x_val shape:  (80, 1, 64, 8, 16)\n",
      "x_test shape:  (1520, 1, 64, 8, 16)\n",
      "y_train shape:  (27200,)\n",
      "y_val shape:  (80,)\n",
      "y_test shape:  (1520,)\n",
      "Generating the TensorDataset objects ...\n",
      "Constructing the model ...\n",
      "Model input shape:  [1, 64, 8, 16]\n",
      "Model output shape:  [8]\n",
      "Training the model ...\n",
      "Total number of data points:      27280\n",
      "Number of training data points:   27200\n",
      "Number of validation data points: 80\n",
      "Number of training batches:    54\n",
      "Number of validation batches:  1\n",
      "Batch size:                    512\n",
      "Shape of training input from the dataloader:   torch.Size([512, 1, 64, 8, 16])\n",
      "Shape of training output from the dataloader:  torch.Size([512])\n",
      "Shape of validation input from the dataloader:   torch.Size([80, 1, 64, 8, 16])\n",
      "Shape of validation output from the dataloader:  torch.Size([80])\n",
      "Selected device:  cuda:0\n",
      "The learning rate has an exponential decay rate of 0.90000.\n",
      "Classification problem detected. We will look at accuracies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  98%|██████████████████████████████████████████▉ | 39/40 [44:54<01:09, 69.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 93\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining the model ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[39m# history = train_pytorch_model(\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m#     model, [train_set, val_set], batch_size=1024, loss_str='crossentropy', optimizer_str='adam', \u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m#     optimizer_params={'weight_decay':0.0001}, loss_function_params=None, learnrate=0.1, \u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m#     learnrate_decay_gamma=0.95, epochs=200, validation_patience=1000000, \u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m#     verbose=1, script_before_save=True, saveto=None, num_workers=0)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_model([train_set, val_set], verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)    \n\u001b[1;32m     95\u001b[0m \u001b[39m# Update relevant fields in the k-fold study dictionary\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUpdating the dictinoary for logging ...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/users/edincer22/comp541-project/capg_3dcnn/../src/models_torch.py:1073\u001b[0m, in \u001b[0;36mPyTorchSmartModule.train_model\u001b[0;34m(self, dataset, verbose, script_before_save, saveto, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(\u001b[39mself\u001b[39m, dataset, verbose:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, script_before_save:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, saveto:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m train_pytorch_model(\u001b[39mself\u001b[39;49m, dataset, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_function, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_function_params, \n\u001b[1;32m   1074\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learning_rate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learning_rate_decay_gamma, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_epochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validation_tolerance_epochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validation_data, verbose, script_before_save, saveto, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1075\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\n",
      "File \u001b[0;32m/scratch/users/edincer22/comp541-project/capg_3dcnn/../src/models_torch.py:473\u001b[0m, in \u001b[0;36mtrain_pytorch_model\u001b[0;34m(model, dataset, batch_size, loss_str, optimizer_str, optimizer_params, loss_function_params, learnrate, learnrate_decay_gamma, epochs, validation_patience, validation_data, verbose, script_before_save, saveto, num_workers)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m display_metrics:\n\u001b[1;32m    472\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 473\u001b[0m         trn_metric, num_train_logits \u001b[39m=\u001b[39m _update_metrics_for_batch(\n\u001b[1;32m    474\u001b[0m             predictions, targets, loss_str, classification, regression, verbose, i, epoch, trn_metric, num_train_logits)\n\u001b[1;32m    476\u001b[0m \u001b[39m# Visualization of progressbar within the batch\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m verbose\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/scratch/users/edincer22/comp541-project/capg_3dcnn/../src/models_torch.py:209\u001b[0m, in \u001b[0;36m_update_metrics_for_batch\u001b[0;34m(predictions, targets, loss_str, classification, regression, verbose, batch_num, epoch, metric, num_logits)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of targets:           \u001b[39m\u001b[39m\"\u001b[39m, targets\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    208\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m correct \u001b[39m=\u001b[39m (class_predictions \u001b[39m==\u001b[39;49m target_predictions)\u001b[39m.\u001b[39;49mint()\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    210\u001b[0m num_logits \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m target_predictions\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    211\u001b[0m metric \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m correct\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define input columns\n",
    "input_cols = list(dba.iloc[:,3:].columns)\n",
    "\n",
    "# Hard-code total number of subjects\n",
    "num_subjects = dba['subject'].nunique()\n",
    "\n",
    "ds = k_fold_study['global_downsampling']\n",
    "\n",
    "\n",
    "for k in range(num_subjects):\n",
    "#for k in range(1):\n",
    "    \n",
    "    print(\"\\n#################################################################\")\n",
    "    print(\"Using subject %d for testing ...\" % (k+1))\n",
    "    print(\"#################################################################\\n\")\n",
    "    \n",
    "    subj_for_testing = [k+1]\n",
    "    \n",
    "    # Un-Correct the output feature count (this is buggy behavior and should be fixed)\n",
    "    hparams['out_features'] = 1\n",
    "    \n",
    "    # Get processed data cell\n",
    "    # CWT: N x C x L --> N x C x H x L\n",
    "    print(\"Generating data cell ...\")\n",
    "    data_processed = generate_cell_array(\n",
    "        dba, hparams,\n",
    "        subjects_column=\"subject\", conditions_column=\"gesture\", trials_column=\"trial\",\n",
    "        input_cols=input_cols, output_cols=[\"gesture\"], specific_conditions=None,\n",
    "        input_preprocessor=None,\n",
    "        output_preprocessor=None,\n",
    "        # Convert N x L x C data to N x C x L and then to N x C' x D x H x W where C'=1, D=L, H=8, W=16\n",
    "        input_postprocessor=lambda arr: arr.reshape(arr.shape[0], 1, arr.shape[1], 8, 16),\n",
    "        output_postprocessor = lambda arr:(arr-1).squeeze(), # torch CrossEntropyLoss needs (N,) array of 0-indexed class labels\n",
    "        subjects_for_testing=subj_for_testing, \n",
    "        trials_for_testing=None,\n",
    "        input_scaling=False, output_scaling=False, input_forward_facing=True, output_forward_facing=True, \n",
    "        data_squeezed=False,\n",
    "        input_towards_future=False, output_towards_future=False, \n",
    "        output_include_current_timestep=True,\n",
    "        use_filtered_data=False, #lpcutoff=CUTOFF, lporder=FILT_ORDER, lpsamplfreq=SAMPL_FREQ,\n",
    "        return_data_arrays_orig=False,\n",
    "        return_data_arrays_processed=False,\n",
    "        return_train_val_test_arrays=False,\n",
    "        return_train_val_test_data=True,\n",
    "        verbosity=1\n",
    "    )\n",
    "    \n",
    "    # Correct the output feature count (this is buggy behavior and should be fixed)\n",
    "    hparams['out_features'] = 8\n",
    "    \n",
    "    print(\"Extracting downsampled input and output data from the datacell ...\")\n",
    "    # Inputs MUST have correct shape\n",
    "    x_train = data_processed[\"x_train\"][::ds]\n",
    "    x_val = data_processed[\"x_val\"][::ds]\n",
    "    x_test = data_processed[\"x_test\"][::ds]\n",
    "    # Outputs MUST be zero-indexed class labels\n",
    "    y_train = data_processed[\"y_train\"][::ds]\n",
    "    y_val = data_processed[\"y_val\"][::ds]\n",
    "    y_test = data_processed[\"y_test\"][::ds]\n",
    "    print(\"x_train shape: \", x_train.shape)\n",
    "    print(\"x_val shape: \", x_val.shape)\n",
    "    print(\"x_test shape: \", x_test.shape)\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "    print(\"y_val shape: \", y_val.shape)\n",
    "    print(\"y_test shape: \", y_test.shape)\n",
    "    del data_processed\n",
    "    # Make datasets from training, validation and test sets\n",
    "    print(\"Generating the TensorDataset objects ...\")\n",
    "    train_set = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "    val_set = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "    test_set = TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).long())\n",
    "    \n",
    "    # If it is the first iteration of the loop, save the hyperparameters dictionary in the k-fold study dictionary\n",
    "    if k==0:\n",
    "        k_fold_study['hparams'] = hparams\n",
    "    \n",
    "    # Construct model\n",
    "    print(\"Constructing the model ...\")\n",
    "    hparams['input_shape'] = list(x_train.shape[1:])\n",
    "    hparams['output_shape'] = [8]\n",
    "    print(\"Model input shape: \", hparams['input_shape'])\n",
    "    print(\"Model output shape: \", hparams['output_shape'])\n",
    "    model = EMGNet(hparams)\n",
    "    if k == 0: print(model)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training the model ...\")\n",
    "    # history = train_pytorch_model(\n",
    "    #     model, [train_set, val_set], batch_size=1024, loss_str='crossentropy', optimizer_str='adam', \n",
    "    #     optimizer_params={'weight_decay':0.0001}, loss_function_params=None, learnrate=0.1, \n",
    "    #     learnrate_decay_gamma=0.95, epochs=200, validation_patience=1000000, \n",
    "    #     verbose=1, script_before_save=True, saveto=None, num_workers=0)\n",
    "    history = model.train_model([train_set, val_set], verbose=1)    \n",
    "    \n",
    "    # Update relevant fields in the k-fold study dictionary\n",
    "    print(\"Updating the dictinoary for logging ...\")\n",
    "    k_fold_study['history_training_loss'].append(history[\"training_loss\"])\n",
    "    k_fold_study[\"history_validation_loss\"].append(history[\"validation_loss\"])\n",
    "    k_fold_study[\"history_training_metrics\"].append(history[\"training_metrics\"])\n",
    "    k_fold_study[\"history_validation_metrics\"].append(history[\"validation_metrics\"])\n",
    "    k_fold_study[\"training_accuracies\"].append(history[\"training_metrics\"][-1])\n",
    "    k_fold_study[\"validation_accuracies\"].append(history[\"validation_metrics\"][-1])\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    print(\"Evaluating the model on the test set ...\")\n",
    "    # results = evaluate_pytorch_model(model, test_set, loss_str='crossentropy', loss_function_params=None,\n",
    "    # batch_size=1024, device_str=\"cuda\", verbose=True, num_workers=0)\n",
    "    results = model.evaluate_model(test_set, verbose=True)\n",
    "    k_fold_study[\"testset_accuracies\"].append(results[\"metrics\"])\n",
    "    print(\"Done with this fold of the K-fold study.\")\n",
    "\n",
    "print(\"Done with the K-fold study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 1, 64, 8, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/users/edincer22/comp541-project/capg_3dcnn'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"capg_3dcnn_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load(\"capg_3dcnn_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMGNet(\n",
       "  (prep_block): Sequential(\n",
       "    (0): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (main_block): Conv_Network(\n",
       "    (net): Sequential(\n",
       "      (0): Conv_Block(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "          (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): Conv_Block(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (2): Conv_Block(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (3): Conv_Block(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): AdaptiveAvgPool3d(output_size=[1, 1, 1])\n",
       "        )\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=64, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving k-fold study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dumping the JSON file ...\")\n",
    "json.dump(k_fold_study, open(make_path(\"../results/\"+hparams['model_name']+\"/k_fold_study.json\"), \"w\"), indent=4)\n",
    "print(\"Saved the JSON file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving general statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving the general statistics ...\")\n",
    "trn_acc_arr = np.array(k_fold_study[\"training_accuracies\"])\n",
    "val_acc_arr = np.array(k_fold_study[\"validation_accuracies\"])\n",
    "tst_acc_arr = np.array(k_fold_study[\"testset_accuracies\"])\n",
    "general_dict = {\"training_accuracy\":trn_acc_arr, \"validation_accuracy\":val_acc_arr, \"testset_accuracy\":tst_acc_arr}\n",
    "general_results = pd.DataFrame(general_dict)\n",
    "print(\"Description of general results:\")\n",
    "general_results_describe = general_results.describe()\n",
    "display(general_results_describe)\n",
    "general_results_describe.to_csv(\n",
    "    make_path(\"../results/\"+hparams['model_name']+\"/general_results.csv\"), header=True, index=True)\n",
    "print(\"Saved general statistics.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import json\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_fold_study = json.load(open(\"../results/capg_replica_dba_v002_2023_01_07_20_07_25/k_fold_study.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting the taining curve ...\")\n",
    "train_loss = np.array(k_fold_study[\"history_training_loss\"])\n",
    "val_loss = np.array(k_fold_study[\"history_validation_loss\"])\n",
    "train_acc = np.array(k_fold_study[\"history_training_metrics\"])\n",
    "val_acc = np.array(k_fold_study[\"history_validation_metrics\"])\n",
    "\n",
    "print(\"Shape of train_loss: \", train_loss.shape)\n",
    "\n",
    "train_loss_mean = np.mean(train_loss, axis=0)\n",
    "train_loss_std = np.std(train_loss, axis=0)# / 2\n",
    "val_loss_mean = np.mean(val_loss, axis=0)\n",
    "val_loss_std = np.std(val_loss, axis=0)# / 2\n",
    "train_acc_mean = np.mean(train_acc, axis=0)\n",
    "train_acc_std = np.std(train_acc, axis=0)# / 2\n",
    "val_acc_mean = np.mean(val_acc, axis=0)\n",
    "val_acc_std = np.std(val_acc, axis=0)# / 2\n",
    "\n",
    "print(\"Shape of train_loss_mean: \", train_loss_mean.shape)\n",
    "print(\"Shape of train_loss_std: \", train_loss_std.shape)\n",
    "\n",
    "epochs = train_loss_mean.shape[0]\n",
    "epochs = np.arange(1, epochs+1)\n",
    "plt.figure(figsize=(8,8), dpi=100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.grid(True)\n",
    "plt.plot(epochs, train_loss_mean, label=\"Training\", color=\"blue\")\n",
    "plt.fill_between(epochs, train_loss_mean-train_loss_std, train_loss_mean+train_loss_std, \n",
    "                 color='blue', alpha=0.2)\n",
    "plt.plot(epochs, val_loss_mean, label=\"Validation\", color=\"orange\")\n",
    "plt.fill_between(epochs, val_loss_mean-val_loss_std, val_loss_mean+val_loss_std,\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.grid(True)\n",
    "plt.plot(epochs, train_acc_mean, color=\"blue\")\n",
    "plt.fill_between(epochs, train_acc_mean-train_acc_std, train_acc_mean+train_acc_std,\n",
    "                 color='blue', alpha=0.2)\n",
    "plt.plot(epochs, val_acc_mean, color=\"orange\")\n",
    "plt.fill_between(epochs, val_acc_mean-val_acc_std, val_acc_mean+val_acc_std,\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.savefig(make_path(\"../results/\"+k_fold_study['hparams']['model_name']+\"/training_history.png\"), dpi=300)\n",
    "\n",
    "print(\"Done plotting the training curve.\")\n",
    "print(\"ALL DONE. GOOD BYE!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the TL part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 13:49:34) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76c97409fe1b9249810273818a125b7bb9a089b64605bbece0f2179e5c237ad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
