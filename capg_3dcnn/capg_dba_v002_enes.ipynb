{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition with CAPG DB-a Dataset Using 3D CNN with EMGNet Architecture (one trial for testing)\n",
    "\n",
    "In this preliminary effort, we will try to perform hand gesture recognition on CAPG DBA dataset.\n",
    "We will use the EMGNet architecture and training procedure, but instead of CWT, we will use 3D CNN on sequences of 2D images.\n",
    "\n",
    "In this version:\n",
    "\n",
    "- EMG data is normalized with the recorded MVC data\n",
    "- The **EMGNet** architecture will be used, along with the training procedure.\n",
    "- A **3D CNN** architecture will be adopted into the EMGNet architecture.\n",
    "- **Raw EMG data** will be used, there will be no preproccessing or feature engineering.\n",
    "- **Training data:** 9 trials per subject per gesture\n",
    "- **Test data:** 1 trial per subject per gesture\n",
    "- K-fold cross-validation will be performed.\n",
    "\n",
    "**NOTE** This code has been tested with:\n",
    "```\n",
    "    numpy version:        1.23.5\n",
    "    scipy version:        1.9.3\n",
    "    sklearn version:      1.2.0\n",
    "    seaborn version:      0.12.1\n",
    "    pandas version:       1.5.2\n",
    "    torch version:        1.12.1+cu113\n",
    "    matplotlib version:   3.6.2\n",
    "    CUDA version:         11.2\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Preliminaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "direc = os.getcwd()\n",
    "print(\"Current Working Directory is: \", direc)\n",
    "KUACC = False\n",
    "if \"scratch\" in direc: # We are using the cluster\n",
    "    KUACC = True\n",
    "    homedir = os.path.expanduser(\"~\")\n",
    "    os.chdir(os.path.join(homedir,\"REPO/comp541-project/capg_3dcnn/\"))\n",
    "    direc = os.getcwd()\n",
    "    print(\"Current Working Directory is now: \", direc)\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../data/\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets_torch import *\n",
    "from models_torch import *\n",
    "from utils_torch import *\n",
    "from cwt import calculate_wavelet_vector, calculate_wavelet_dataset\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import statistics\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "# Print versions\n",
    "print(\"numpy version:       \", np.__version__)\n",
    "print(\"scipy version:       \", sp.__version__)\n",
    "print(\"sklearn version:     \", sklearn.__version__)\n",
    "print(\"seaborn version:     \", sns.__version__)\n",
    "print(\"pandas version:      \", pd.__version__)\n",
    "print(\"torch version:       \", torch.__version__)\n",
    "print(\"matplotlib version:  \", matplotlib.__version__)\n",
    "\n",
    "\n",
    "# Checking to see if CUDA is available for us\n",
    "print(\"Checking to see if PyTorch recognizes GPU...\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Whether to use latex rendering in plots throughout the notebook\n",
    "USE_TEX = False\n",
    "FONT_SIZE = 12\n",
    "\n",
    "# Setting matplotlib plotting variables\n",
    "if USE_TEX:\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.size\": FONT_SIZE,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Computer Modern Roman\"]\n",
    "    })\n",
    "else:\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.size\": FONT_SIZE,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\"]\n",
    "    })\n",
    "\n",
    "# Do not plot figures inline (only useful for cluster)\n",
    "%matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Hyperparameters and Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General settings of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_study = {\n",
    "    'code':'capg_3dcnn/capg_dba_v002',\n",
    "    'package':'torch',\n",
    "    'dataset':'capg',\n",
    "    'subdataset':'dba',\n",
    "    \"training_accuracies\": [],\n",
    "    \"validation_accuracies\": [],\n",
    "    \"testset_accuracies\": [],\n",
    "    \"history_training_loss\": [],\n",
    "    \"history_training_metrics\": [],\n",
    "    \"history_validation_loss\": [],\n",
    "    \"history_validation_metrics\": [],\n",
    "    \"preprocessing\":None,\n",
    "    \"feature_engineering\":None,\n",
    "    \"k_fold_mode\":\"1 trial for testing\",\n",
    "    \"global_downsampling\":10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"model_name\": autoname(\"capg_3dcnn_dba_v002\"),\n",
    "    # General hyperparameters\n",
    "    \"in_features\": 128,\n",
    "    \"out_features\": 1,\n",
    "    # Sequence hyperparameters\n",
    "    \"in_seq_len_sec\": 0.16,\n",
    "    \"out_seq_len_sec\": 0,\n",
    "    \"data_sampling_rate_Hz\": 1000.0,\n",
    "    \"data_downsampling\": 5,\n",
    "    \"sequence_downsampling\": 1,\n",
    "    \"in_seq_len\": 0,\n",
    "    \"out_seq_len\": 0,\n",
    "    # Convolution blocks\n",
    "    \"num_conv_blocks\": 4,\n",
    "    \"conv_dim\": 3,\n",
    "    \"conv_params\": None,\n",
    "    \"conv_channels\": [16, 32, 32, 64],\n",
    "    \"conv_kernel_size\": 3,\n",
    "    \"conv_padding\": \"same\",\n",
    "    \"conv_stride\": 1,\n",
    "    \"conv_dilation\": 1,\n",
    "    \"conv_activation\": \"ReLU\",\n",
    "    \"conv_activation_params\": None,#{\"negative_slope\": 0.1},\n",
    "    \"conv_norm_layer_type\": \"BatchNorm\",\n",
    "    \"conv_norm_layer_position\": \"before\",\n",
    "    \"conv_norm_layer_params\": None,\n",
    "    \"conv_dropout\": None,\n",
    "    \"pool_type\": [None, None, None, \"AdaptiveAvg\"],\n",
    "    \"pool_kernel_size\": 2,\n",
    "    \"pool_padding\": 0,\n",
    "    \"pool_stride\": 1,\n",
    "    \"pool_dilation\": 1,\n",
    "    \"pool_params\": None,\n",
    "    \"min_image_size\": 1,\n",
    "    \"adaptive_pool_output_size\": [1,1,1],\n",
    "    # Fully connected blocks\n",
    "    \"dense_width\": \"auto\",\n",
    "    \"dense_depth\": 0,\n",
    "    \"dense_activation\": \"ReLU\",\n",
    "    \"dense_activation_params\": None,\n",
    "    \"output_activation\": None,\n",
    "    \"output_activation_params\": None,\n",
    "    \"dense_norm_layer_type\": None,\n",
    "    \"dense_norm_layer_position\": None,\n",
    "    \"dense_norm_layer_params\": None,\n",
    "    \"dense_dropout\": None,\n",
    "    # Training procedure\n",
    "    \"l2_reg\": 0.0001,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 60,\n",
    "    \"validation_data\": [0.05,'testset'],\n",
    "    \"validation_tolerance_epochs\": 1000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"learning_rate_decay_gamma\": 0.9,\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"optimizer_params\": None\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/CAPG/parquet\"\n",
    "def load_single_capg_dataset(data_dir, db_str:str=\"dba\"):\n",
    "    data_lst = []\n",
    "    for i,file in enumerate(os.listdir(data_dir)):\n",
    "        if file.endswith(\".parquet\") and db_str in file:\n",
    "            print(\"Loading file: \", file)\n",
    "            data_lst.append(pd.read_parquet(os.path.join(data_dir, file)))\n",
    "    data = pd.concat(data_lst, axis=0, ignore_index=True)\n",
    "    return data\n",
    "dba_tot = load_single_capg_dataset(data_dir, db_str=\"dba\")\n",
    "dba_mvc = dba_tot.loc[dba_tot[\"gesture\"].isin([100, 101])]\n",
    "dba = dba_tot.loc[~dba_tot[\"gesture\"].isin([100, 101])]\n",
    "print(\"dba_tot shape: \", dba_tot.shape)\n",
    "print(\"dba_mvc shape: \", dba_mvc.shape)\n",
    "print(\"dba shape: \", dba.shape)\n",
    "print(\"Columns: \")\n",
    "print(dba_tot.columns)\n",
    "print(\"Description: \")\n",
    "print(dba.iloc[:,:3].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize EMG Data\n",
    "\n",
    "Here the recorded MVC values will be used for normalizaing EMG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mvc = dba_mvc.iloc[:,3:].max(axis=0)\n",
    "del dba_mvc\n",
    "# print(\"max_mvc for 5 first channels: \")\n",
    "# print(max_mvc[:5])\n",
    "# print(\"shape of max_mvc: \", max_mvc.shape)\n",
    "# print(\"max of dba before normalization: (first five)\")\n",
    "# print(dba.iloc[:,3:].max(axis=0)[:5])\n",
    "dba.iloc[:,3:] = dba.iloc[:,3:].div(max_mvc, axis=1)\n",
    "# print(\"max of dba_norm after normalization: \")\n",
    "# print(dba_norm.iloc[:,3:].max(axis=0)[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- k-fold study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMGNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMGNet(PyTorchSmartModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(EMGNet, self).__init__(hparams)\n",
    "        self.prep_block = nn.Sequential(\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main_block = Conv_Network(hparams)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep_block(x)\n",
    "        x = self.main_block(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform k-fold cross-validation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input columns\n",
    "input_cols = list(dba.iloc[:,3:].columns)\n",
    "\n",
    "# Hard-code total number of trials\n",
    "NUM_TRIALS = 10\n",
    "\n",
    "ds = k_fold_study['global_downsampling']\n",
    "\n",
    "for k in range(NUM_TRIALS):\n",
    "    \n",
    "    print(\"\\n#################################################################\")\n",
    "    print(\"Using trial %d for testing ...\" % (k+1))\n",
    "    print(\"#################################################################\\n\")\n",
    "    \n",
    "    trial_for_testing = [k+1]\n",
    "    \n",
    "    # Un-Correct the output feature count (this is buggy behavior and should be fixed)\n",
    "    hparams['out_features'] = 1\n",
    "    \n",
    "    # Get processed data cell\n",
    "    # CWT: N x C x L --> N x C x H x L\n",
    "    print(\"Preparing the datacell ...\")\n",
    "    data_processed = generate_cell_array(\n",
    "        dba, hparams,\n",
    "        subjects_column=\"subject\", conditions_column=\"gesture\", trials_column=\"trial\",\n",
    "        input_cols=input_cols, output_cols=[\"gesture\"], specific_conditions=None,\n",
    "        input_preprocessor=None,\n",
    "        output_preprocessor=None,\n",
    "        # Convert N x L x C data to N x C x L and then to N x C' x D x H x W where C'=1, D=L, H=8, W=16\n",
    "        input_postprocessor=lambda arr: arr.reshape(arr.shape[0], 1, arr.shape[1], 8, 16),\n",
    "        output_postprocessor = lambda arr:(arr-1).squeeze(), # torch CrossEntropyLoss needs (N,) array of 0-indexed class labels\n",
    "        subjects_for_testing=None, \n",
    "        trials_for_testing=trial_for_testing,\n",
    "        input_scaling=False, output_scaling=False, input_forward_facing=True, output_forward_facing=True, \n",
    "        data_squeezed=False,\n",
    "        input_towards_future=False, output_towards_future=False, \n",
    "        output_include_current_timestep=True,\n",
    "        use_filtered_data=False, #lpcutoff=CUTOFF, lporder=FILT_ORDER, lpsamplfreq=SAMPL_FREQ,\n",
    "        return_data_arrays_orig=False,\n",
    "        return_data_arrays_processed=False,\n",
    "        return_train_val_test_arrays=False,\n",
    "        return_train_val_test_data=True,\n",
    "        verbosity=1\n",
    "    )\n",
    "    \n",
    "    # Correct the output feature count (this is buggy behavior and should be fixed)\n",
    "    hparams['out_features'] = 8\n",
    "    \n",
    "    print(\"Generating downsampled input and output from datacell ...\")\n",
    "    # Inputs MUST have correct shape\n",
    "    x_train = data_processed[\"x_train\"][::ds]\n",
    "    x_val = data_processed[\"x_val\"][::ds]\n",
    "    x_test = data_processed[\"x_test\"][::ds]\n",
    "    # Outputs MUST be zero-indexed class labels\n",
    "    y_train = data_processed[\"y_train\"][::ds]\n",
    "    y_val = data_processed[\"y_val\"][::ds]\n",
    "    y_test = data_processed[\"y_test\"][::ds]\n",
    "    print(\"x_train shape: \", x_train.shape)\n",
    "    print(\"x_val shape: \", x_val.shape)\n",
    "    print(\"x_test shape: \", x_test.shape)\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "    print(\"y_val shape: \", y_val.shape)\n",
    "    print(\"y_test shape: \", y_test.shape)\n",
    "    del data_processed\n",
    "    # Make datasets from training, validation and test sets\n",
    "    print(\"Generating TensorDatsets form the data ...\")\n",
    "    train_set = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "    val_set = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "    test_set = TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).long())\n",
    "    \n",
    "    # If it is the first iteration of the loop, save the hyperparameters dictionary in the k-fold study dictionary\n",
    "    if k==0:\n",
    "        k_fold_study['hparams'] = hparams\n",
    "    \n",
    "    # Construct model\n",
    "    print(\"Constructing model ...\")\n",
    "    hparams['input_shape'] = x_train.shape[1:]\n",
    "    hparams['output_shape'] = [8]\n",
    "    print(\"Model input shape: \", hparams['input_shape'])\n",
    "    print(\"Model output shape: \", hparams['output_shape'])\n",
    "    model = EMGNet(hparams)\n",
    "    if k == 0: print(model)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model ...\")\n",
    "    # history = train_pytorch_model(\n",
    "    #     model, [train_set, val_set], batch_size=1024, loss_str='crossentropy', optimizer_str='adam', \n",
    "    #     optimizer_params={'weight_decay':0.0001}, loss_function_params=None, learnrate=0.1, \n",
    "    #     learnrate_decay_gamma=0.95, epochs=200, validation_patience=1000000, \n",
    "    #     verbose=1, script_before_save=True, saveto=None, num_workers=0)\n",
    "    history = model.train_model([train_set, val_set], verbose=1)    \n",
    "    \n",
    "    # Update relevant fields in the k-fold study dictionary\n",
    "    print(\"Updating dictionary for logging ...\")\n",
    "    k_fold_study['history_training_loss'].append(history[\"training_loss\"])\n",
    "    k_fold_study[\"history_validation_loss\"].append(history[\"validation_loss\"])\n",
    "    k_fold_study[\"history_training_metrics\"].append(history[\"training_metrics\"])\n",
    "    k_fold_study[\"history_validation_metrics\"].append(history[\"validation_metrics\"])\n",
    "    k_fold_study[\"training_accuracies\"].append(history[\"training_metrics\"][-1])\n",
    "    k_fold_study[\"validation_accuracies\"].append(history[\"validation_metrics\"][-1])\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    print(\"Evaluating model on test set ...\")\n",
    "    # results = evaluate_pytorch_model(model, test_set, loss_str='crossentropy', loss_function_params=None,\n",
    "    # batch_size=1024, device_str=\"cuda\", verbose=True, num_workers=0)\n",
    "    results = model.evaluate_model(test_set, verbose=True)\n",
    "    k_fold_study[\"testset_accuracies\"].append(results[\"metrics\"])\n",
    "    print(\"Done with this fold of the K-fold study.\")\n",
    "\n",
    "print(\"Done with the K-fold study.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving k-fold study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dumping the JSON file ...\")\n",
    "json.dump(k_fold_study, open(make_path(\"../results/\"+hparams['model_name']+\"/k_fold_study.json\"), \"w\"), indent=4)\n",
    "print(\"Saved the JSON file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving general statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving the general statistics ...\")\n",
    "trn_acc_arr = np.array(k_fold_study[\"training_accuracies\"])\n",
    "val_acc_arr = np.array(k_fold_study[\"validation_accuracies\"])\n",
    "tst_acc_arr = np.array(k_fold_study[\"testset_accuracies\"])\n",
    "general_dict = {\"training_accuracy\":trn_acc_arr, \"validation_accuracy\":val_acc_arr, \"testset_accuracy\":tst_acc_arr}\n",
    "general_results = pd.DataFrame(general_dict)\n",
    "print(\"Description of general results:\")\n",
    "general_results_describe = general_results.describe()\n",
    "display(general_results_describe)\n",
    "general_results_describe.to_csv(\n",
    "    make_path(\"../results/\"+hparams['model_name']+\"/general_results.csv\"), header=True, index=True)\n",
    "print(\"Saved general statistics.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import json\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_fold_study = json.load(open(\"../results/capg_replica_dba_v002_2023_01_07_20_07_25/k_fold_study.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting training curve ...\")\n",
    "train_loss = np.array(k_fold_study[\"history_training_loss\"])\n",
    "val_loss = np.array(k_fold_study[\"history_validation_loss\"])\n",
    "train_acc = np.array(k_fold_study[\"history_training_metrics\"])\n",
    "val_acc = np.array(k_fold_study[\"history_validation_metrics\"])\n",
    "\n",
    "print(\"Shape of train_loss: \", train_loss.shape)\n",
    "\n",
    "train_loss_mean = np.mean(train_loss, axis=0)\n",
    "train_loss_std = np.std(train_loss, axis=0)# / 2\n",
    "val_loss_mean = np.mean(val_loss, axis=0)\n",
    "val_loss_std = np.std(val_loss, axis=0)# / 2\n",
    "train_acc_mean = np.mean(train_acc, axis=0)\n",
    "train_acc_std = np.std(train_acc, axis=0)# / 2\n",
    "val_acc_mean = np.mean(val_acc, axis=0)\n",
    "val_acc_std = np.std(val_acc, axis=0)# / 2\n",
    "\n",
    "print(\"Shape of train_loss_mean: \", train_loss_mean.shape)\n",
    "print(\"Shape of train_loss_std: \", train_loss_std.shape)\n",
    "\n",
    "epochs = train_loss_mean.shape[0]\n",
    "epochs = np.arange(1, epochs+1)\n",
    "plt.figure(figsize=(8,8), dpi=100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.grid(True)\n",
    "plt.plot(epochs, train_loss_mean, label=\"Training\", color=\"blue\")\n",
    "plt.fill_between(epochs, train_loss_mean-train_loss_std, train_loss_mean+train_loss_std, \n",
    "                 color='blue', alpha=0.2)\n",
    "plt.plot(epochs, val_loss_mean, label=\"Validation\", color=\"orange\")\n",
    "plt.fill_between(epochs, val_loss_mean-val_loss_std, val_loss_mean+val_loss_std,\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.grid(True)\n",
    "plt.plot(epochs, train_acc_mean, color=\"blue\")\n",
    "plt.fill_between(epochs, train_acc_mean-train_acc_std, train_acc_mean+train_acc_std,\n",
    "                 color='blue', alpha=0.2)\n",
    "plt.plot(epochs, val_acc_mean, color=\"orange\")\n",
    "plt.fill_between(epochs, val_acc_mean-val_acc_std, val_acc_mean+val_acc_std,\n",
    "                 color='orange', alpha=0.2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.savefig(make_path(\"../results/\"+k_fold_study['hparams']['model_name']+\"/training_history.png\"), dpi=300)\n",
    "\n",
    "print(\"Done plotting the training curve.\")\n",
    "print(\"ALL DONE. GOOD BYE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:57:06) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "297a68ee4069fbf732949d9b21475f7c611567aa29a5760dd0fd474131db8400"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
